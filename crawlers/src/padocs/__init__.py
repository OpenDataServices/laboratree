import os
import csv
import dataset


def seed_url_db(context, data):
    """
    Queries EPDS database.
    Expects context params:
     - start
     - end
     - area
    """
    start = context.get('start')
    end = context.get('end')
    area = context.get('area')

    db = dataset.connect(os.environ.get('EPDS_DB_URL'))
    query = """SELECT url FROM planit_key_fields
    WHERE area_name = '%s' and start_date>='%s' and start_date<='%s'""" % (area, start, end)
    result = db.query(query)
    for r in result:
        context.emit(data={'url': r['url']})


def seed_url_csv(context, data):
    """
    Reads CSV files from src/seed_urls/ directory.
    CSVs are single column, line-separated lists of URLs
    generated by a separate out-of-band process.
    Expects context params:
     - file (inc extension)
    """
    file = os.path.join(os.path.dirname(__file__), 'seed_urls/%s' % context.params.get('file'))
    with open(file, mode='r') as f:
        urls = csv.reader(f)

        for url in urls:
            context.emit(data={'url': url[0]})


def aleph_process(context, data):

    out = {
        'source_url': data.get('source_url', data.get('url')),
        'area_name': context.crawler.name,
        'document_title': data.get('title', ''),
        'document_file_name': data.get('file_name'),
        'aleph_document_id': data.get('aleph_id'),
        'aleph_collection_id': data.get('aleph_collection_id')
    }
    context.emit(data=out)


def export(context, params):
    local_db = dataset.connect(os.environ.get('MEMORIOUS_DATASTORE_URI'))
    from_table = params.get('table')
    data = local_db[from_table].all()

    remote_db = dataset.connect(os.environ.get('EPDS_DB_URL'))
    remote_db['filtered_pa_documents'].upsert_many(data, ['area_name', 'aleph_document_id'])


def dump(context, data):
    print(data)